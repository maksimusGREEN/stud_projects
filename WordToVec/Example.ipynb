{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vladimir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.preproc_functions import *\n",
    "from utils.model import Word2Vec, CustomDataset\n",
    "from utils.embedding_trainer import train_word2vec\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lenta data from source\n",
    "corpus = get_corpus(num_doc=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:09<00:00, 153.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data and vocab from corpus\n",
    "preproc_corpus = corpus_prepros(corpus)\n",
    "data, vocab = data_preparation(preproc_corpus, method='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss:  7.965456402063736 ||| Validation Loss:  7.476635755341628\n",
      "Epoch 2/10: Train Loss:  7.044787921297568 ||| Validation Loss:  7.16904864804498\n",
      "Epoch 3/10: Train Loss:  6.737176429657709 ||| Validation Loss:  7.042442236275509\n",
      "Epoch 4/10: Train Loss:  6.560011132887798 ||| Validation Loss:  6.978517044001612\n",
      "Epoch 5/10: Train Loss:  6.439085873590637 ||| Validation Loss:  6.94392278605494\n",
      "Epoch 6/10: Train Loss:  6.345834787540172 ||| Validation Loss:  6.925826810968333\n",
      "Epoch 7/10: Train Loss:  6.26879526576322 ||| Validation Loss:  6.913416102836872\n",
      "Epoch 8/10: Train Loss:  6.201809193864579 ||| Validation Loss:  6.903815118197737\n",
      "Epoch 9/10: Train Loss:  6.142847096132609 ||| Validation Loss:  6.8980178339727996\n",
      "Epoch 10/10: Train Loss:  6.0906869780999 ||| Validation Loss:  6.89230867254323\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Split the data into training and test sets\n",
    "split_index = int(len(data) * 0.90)\n",
    "train_dataset = CustomDataset(data[:split_index])\n",
    "valid_dataset = CustomDataset(data[split_index:])\n",
    "\n",
    "# Define dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define and train model\n",
    "model = Word2Vec(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "model = train_word2vec(model, train_dataloader, valid_dataloader, device, epochs=10, learning_rate=0.001)\n",
    "\n",
    "# Get result dict\n",
    "params = list(model.parameters())\n",
    "word_vectors = params[0].detach()\n",
    "unique_words = list(vocab.keys())\n",
    "word_dict = {word: vector.cpu().numpy() for word, vector in zip(unique_words, word_vectors)} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
